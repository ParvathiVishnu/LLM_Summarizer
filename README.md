# ğŸ“„ LLM-Powered Document Summarizer using Ollama & Streamlit

A local privacy-preserving app that extracts and summarizes documents (`.txt`, `.pdf`, `.docx`) using your own LLM (`llama3.2-summary`) via [Ollama](https://ollama.com/). Built with Python, Streamlit, and a custom fine-tuned model for concise, bullet-point summaries.

---

## Features

- Extracts text from `.txt`, `.pdf`, and `.docx`
- Summarizes large documents in chunks (~1500 characters)
- Outputs clean, bullet-style summaries using `llama3.2-summary`
- Allows summary download as `.txt`
- Batch summarization + comparison view
- 100% local and offline â€“ no OpenAI or cloud dependency

---

## Dependencies

Install via `requirements.txt` or manually:

```bash
pip install streamlit docx2txt pytesseract pdf2image Pillow PyPDF2
```

> Windows users may also need to install poppler for `pdf2image`.

---

## Setup Instructions

### 1. Clone the repo

```bash
git clone https://github.com/your-username/llm-document-summarizer.git
cd llm-document-summarizer
```

### 2. Create a Python environment

```bash
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Start Ollama and pull base model

Make sure youâ€™ve installed [Ollama](https://ollama.com/download). Then:

```bash
ollama run llama3.2
```

### 5. Create custom summarizer model

Make a `Modelfile` like this:

```Dockerfile
FROM llama3:8b

SYSTEM "You are an expert summarizer. Summarize any input into clear, concise bullet points. Focus only on the most important and relevant information. Eliminate redundancy and preserve the original meaning."

PARAMETER temperature 0.3
```

Then run:

```bash
ollama create llama3.2-summary -f Modelfile
```

---

## Run the App

```bash
streamlit run streamlit_app.py
```

It will launch in your browser.

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ compare_results.py
â”œâ”€â”€ batch_tests.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ ollama_helper.py
â”œâ”€â”€ test_documents/
â”œâ”€â”€ test_results_llama3.2-summary/
â””â”€â”€ llama3.2-summary/
    Modelfile
```

---

## App Flow (Mermaid Diagram)

```mermaid
flowchart TD
    A[User uploads document:\nTXT, PDF, or DOCX] --> B[Extract text using utils.py]
    B --> C[Split text into chunks\napprox 1500 characters]
    C --> D[Summarize each chunk\nwith llama3.2-summary model]
    D --> E[Clean summaries:\nremove markdown and fix bullets]
    E --> F[Combine chunk summaries\ninto final summary]
    F --> G[Show summary in Streamlit\n+ allow download]
    F --> H[Save summary to\n test_results_llama3.2-summary/]
    H --> I[Compare summary vs original\nwith compare_results.py]
```

---

## ğŸ“¸ Screenshots

> *(Add screenshots here to show UI, summary output, etc.)*

---

## ğŸ“Œ Sample Prompt to LLM

```
Summarize the following document content in clear, concise bullet points. 
Focus only on the key points while preserving semantic meaning:
```

---

## ğŸ” Why Local?

- No API keys required
- Full privacy â€” documents stay on your machine
- Custom system prompts for consistent summarization

---

## ğŸ™‹â€â™€ï¸ Future Improvements

- Add extractive vs abstractive summary toggle
- Summarization quality metrics (e.g., Rouge / BLEU)
- OCR improvements for scanned PDFs
- Frontend with filtering by summary length

---

## ğŸ“„ License

MIT License â€” free to use, modify, and build on.

---

## ğŸ‘©â€ğŸ’» Author

Built by **Parvathi Vishnu** â€” feel free to connect on [LinkedIn](https://www.linkedin.com/) or explore more on [GitHub](https://github.com/your-username).

